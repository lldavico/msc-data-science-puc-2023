{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/google-research/bert\n",
        "\n",
        "https://arxiv.org/pdf/1810.04805.pdf"
      ],
      "metadata": {
        "id": "fv1Or2icq8K8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We introduce a new language representation model called BERT, which stands for\n",
        "Bidirectional Encoder Representations from\n",
        "Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from\n",
        "unlabeled text by jointly conditioning on both\n",
        "left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\n",
        "to create state-of-the-art models for a wide\n",
        "range of tasks, such as question answering and\n",
        "language inference, without substantial taskspecific architecture modifications."
      ],
      "metadata": {
        "id": "t71ZzrCYrOP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comprensión de Contexto Bidireccional\n",
        "A diferencia de los modelos tradicionales que leen secuencias de texto en una dirección (ya sea de izquierda a derecha o de derecha a izquierda), BERT lee la secuencia completa de palabras de una vez. Este enfoque bidireccional permite que el modelo entienda el contexto de una palabra basándose en todo su entorno (tanto a la izquierda como a la derecha de la palabra).\n",
        "\n",
        "### Diferencias del Transformador Original\n",
        "#### Uso de Solo el Codificador\n",
        "El modelo Transformador original consta de un codificador y un decodificador. BERT utiliza únicamente la pila de codificadores del Transformador.\n",
        "\n",
        "#### Bidireccionalidad\n",
        "El Transformador en su forma original no es inherentemente bidireccional de la manera en que lo es BERT. Procesa el texto de manera secuencial (ya sea de izquierda a derecha o de derecha a izquierda) para tareas como la traducción.\n",
        "\n",
        "#### Enfoque de Preentrenamiento\n",
        "El preentrenamiento de BERT involucra modelado del lenguaje enmascarado y predicción de la siguiente oración, lo cual no es una característica del Transformador original.\n",
        "\n",
        "#### Ajuste Fino para Tareas Específicas\n",
        "El Transformador original no enfatiza el paso de ajuste fino tanto como BERT, que está diseñado para adaptarse a varias tareas de PNL con ajustes mínimos específicos de la tarea.\n"
      ],
      "metadata": {
        "id": "2JszVzissgyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzwhICWitrkQ",
        "outputId": "74b06d90-741a-476b-8dea-4146b5065d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers pandas numpy nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/news.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-urZ134t56v",
        "outputId": "e2b2dbf7-d3ce-41ca-c48c-dc7b7ad5a9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XsJdId65vcgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Limpieza basica del texto, remover puntuación y digitos como fechas, números de usuario de twitter etc.\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+') # Acá es para eligir solo parabras del alfabeto entre A-Z minuscula o mayscula, es una RE\n",
        "lemmatizer = WordNetLemmatizer() #Acá iremos reducir las palabras para su clasificación minima, la raíz semantica de la palabra.\n",
        "\n",
        "\n",
        "primera_noticia = df.iloc[0]\n",
        "def preprocess_text(text):\n",
        "    words = []\n",
        "    for sentence in sent_tokenize(text):\n",
        "        tokens = [word for word in tokenizer.tokenize(sentence)]\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        words += tokens\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['preprocessado'] = df['title'] + \" \" + df['text']\n",
        "df['preprocessado'] = df['preprocessado'].apply(preprocess_text)\n",
        "\n",
        "# Tokenizador de BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preparar datos para BERT\n",
        "def prepare_data_for_bert(texts, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_data = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids.append(encoded_data['input_ids'])\n",
        "        attention_masks.append(encoded_data['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "max_length = 256  # Ajusta esto según la longitud de tus textos\n",
        "input_ids, attention_masks = prepare_data_for_bert(df['preprocessado'], tokenizer, max_length)\n",
        "\n",
        "# Etiquetas\n",
        "labels = torch.tensor(df['authenticity'].apply(lambda x: 0 if x == 'Fake' else 1).values)\n",
        "\n",
        "# Dataset\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Dividir el dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPLVJ5bvt7c_",
        "outputId": "0920d893-41da-4f9a-8de5-a1a521c0ad59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo BERT para clasificación\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2, # Clasificación binaria\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Mover modelo al dispositivo GPU\n",
        "model.cuda()\n",
        "\n",
        "# Configurar el optimizador\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "# https://arxiv.org/pdf/1711.05101.pdf\n",
        "# Total de pasos de entrenamiento es número de batches * número de épocas\n",
        "total_steps = len(train_loader) * 4\n",
        "\n",
        "# Creador del schedule para el learning rate\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = total_steps\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9Hv-9Dmt8uu",
        "outputId": "d0e0a871-16ed-4bc7-d80a-d85e9a0ca1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Número de épocas\n",
        "epochs = 2\n",
        "acumulation_steps = 2\n",
        "# Bucle de entrenamiento\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Entrenar\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "\n",
        "\n",
        "        # Desempaquetar los datos del dataloader\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_input_mask = batch[1].cuda()\n",
        "        b_labels = batch[2].cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Prevenir el problema del exploding gradient\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Actualización de parámetros y step del scheduler\n",
        "        if (step + 1) % acumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "    # Cálculo de la pérdida promedio sobre la época\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Promedio de pérdida en entrenamiento: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluación\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    for batch in val_loader:\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(f\"Exactitud en validación: {eval_accuracy/nb_eval_steps}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz8XBDGUt99O",
        "outputId": "1944b702-fbcc-47d0-f630-b04816443209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de pérdida en entrenamiento: 0.1422567501880694\n",
            "Exactitud en validación: 0.9965277777777778\n",
            "Promedio de pérdida en entrenamiento: 0.0023303805838222618\n",
            "Exactitud en validación: 0.9985119047619048\n"
          ]
        }
      ]
    }
  ]
}