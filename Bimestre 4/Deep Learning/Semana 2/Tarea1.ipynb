{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "073323e66e9baa0029735d58479072d5",
     "grade": false,
     "grade_id": "cell-cab189819af239ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Tarea 1: Deep Learning\n",
    "\n",
    "## Objetivos:\n",
    "### El objetivo de esta tarea es poder demostrar el conocimiento adquirido en las clases 1-5, tutoriales 1-3 y el laboratorio formativo 1.\n",
    "\n",
    "## Instrucciones:\n",
    "- Para esta tarea debe completar las celdas faltantes, además de tomar decisiones sobre ciertos parámetros.\n",
    "- Tendrán ya creada para ustedes un conjunto de entradas y salidas deseadas para una función, \\( F(x,y) = x^2 + y^2 \\), el objetivo principal es que dado un valor, la red sea capaz de predecir el resultado correcto con un umbral de precisión de x%, donde la nota será por puntos en relación a la precisión y error.\n",
    "\n",
    "Por ejemplo, si de 100 predicciones la red creada tiene un 80% de precisión para |y_gorro - y_teoria| <= 0.1, tienes 15pts de 100, cuanto mayor sea la precisión, mayor la nota hasta 80/100 pts.\n",
    "\n",
    "Los últimos 20 pts serán evaluados con base a las decisiones tomadas por ustedes, por esto en la última celda de tests es importante justificar las decisiones tomadas, sea la cantidad de capas ocultas, épocas, tasa de aprendizaje, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cb4649ed9e2489579b5c4353ec007f6",
     "grade": false,
     "grade_id": "cell-cbb97c8f0098f5a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Formato de los datos\n",
    "- Los datos se presentan con 2 valores de entrada y uno de salida: `input_1`, `input_2` y un valor de salida `output`.\n",
    "- En total hay 1000 entradas (1000 `input_1`, 1000 `input_2`, 1000 `output`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb654014e92b72ba614f7ee69174e886",
     "grade": false,
     "grade_id": "cell-ab79091b13b2890c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "class Linear:\n",
    "    def derivative(self, x):\n",
    "        return 1\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "779fbce766fc2eb46951e13aced1ca83",
     "grade": false,
     "grade_id": "cell-bf4317b617a6f369",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation = Sigmoid()):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.output = 0\n",
    "        self.inputs = []\n",
    "        self.error = 0\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        self.output = self.activation(total)\n",
    "        return self.output\n",
    "    \n",
    "    def backpropagate_error(self, error):\n",
    "        self.error = self.activation.derivative(self.output) * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4a69c26473aa40b51ccaf5ec9dadbe",
     "grade": false,
     "grade_id": "cell-5c80ccf78e91be2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_red, output, h_layers, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = []\n",
    "        self.input = input_red\n",
    "        self.__init__layers(h_layers, output)\n",
    "   \n",
    "\n",
    "    \n",
    "    def __init__layers(self, h_layers, output):\n",
    "        for indice_layer in range(len(h_layers)):\n",
    "            \n",
    "                layer = []\n",
    "                for cantidad_de_neuronas in range(h_layers[indice_layer]):\n",
    "                    if indice_layer == 0:\n",
    "                        neurona = Neuron(\n",
    "                            [random.uniform(-1, 1) for _ in range(self.input)], random.uniform(-1, 1)\n",
    "                        )\n",
    "                    else:\n",
    "                        neurona = Neuron(\n",
    "                            [random.uniform(-1, 1) for _ in range(h_layers[indice_layer-1])], random.uniform(-1, 1)\n",
    "                        )\n",
    "                    layer.append(neurona)\n",
    "                self.layers.append(layer)\n",
    "                \n",
    "        self.layers.append([Neuron([random.uniform(-1, 1) for _ in range(h_layers[-1])], random.uniform(-1, 1),Linear() ) for _ in range(output)])\n",
    "    \n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        # your code here        \n",
    "        for i_layer in range(len(self.layers)):\n",
    "            outputs = []\n",
    "            for i_neuron in range(len(self.layers[i_layer])):\n",
    "                outputs.append(self.layers[i_layer][i_neuron].feedforward(inputs))\n",
    "            inputs = outputs # para la proxima capa recibir lo que retorna la capa que acabamos de pasar\n",
    "        return outputs\n",
    "    \n",
    "    def train(self, inputs, targets):\n",
    "        outputs = self.feedforward(inputs)\n",
    "        errors = [targets[i] - outputs[i] for i in range(len(outputs))]\n",
    "    \n",
    "        for indice_layers in range(len(self.layers)-1, -1, -1):\n",
    "            layer = self.layers[indice_layers]\n",
    "            next_layer = self.layers[indice_layers+1] if indice_layers != len(self.layers)-1 else None\n",
    "            for indice_neurona in range(len(layer)):\n",
    "                neuron = layer[indice_neurona]\n",
    "                if next_layer is None:\n",
    "                    neuron.backpropagate_error(errors[indice_neurona])\n",
    "                else:\n",
    "                    error = 0\n",
    "                    for n in next_layer:\n",
    "                        error += n.error * n.weights[indice_neurona]\n",
    "                    neuron.backpropagate_error(error)    \n",
    "\n",
    "        for indice_layers in range(len(self.layers)):\n",
    "            for indice_neurona in range(len(self.layers[indice_layers])):\n",
    "                \n",
    "                neuron = self.layers[indice_layers][indice_neurona]\n",
    "                # your code here\n",
    "                for i_weight in range(len(neuron.weights)):\n",
    "                    neuron.weights[i_weight] += self.learning_rate * neuron.error * neuron.inputs[i_weight]\n",
    "                    neuron.bias += self.learning_rate * neuron.error\n",
    "        error_final = sum([0.5*e**2 for e in errors]) / len(errors)  \n",
    "        return error_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea3d2cdab1d96d9c261c928c4ad2009",
     "grade": false,
     "grade_id": "cell-e9e8ab1934cabf07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 242.3588546039383\n",
      "error: 0.07301374743165766\n",
      "error: 0.023544605072516135\n",
      "error: 0.019484660011387676\n",
      "error: 0.017717141034619115\n",
      "error: 0.01642865975535478\n",
      "error: 0.015390370276072842\n",
      "error: 0.014524555986573304\n",
      "error: 0.01378867116416565\n",
      "error: 0.013154634840884768\n",
      "0.012607429531411762\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Generate random x, y values\n",
    "random.seed(\"01101100 01100101 01101110 01101001 01101110\")\n",
    "\n",
    "# Generate training data for f(x, y) = x^2 + y^2\n",
    "X = [[random.uniform(-1, 1), random.uniform(-1, 1)] for _ in range(1000)]\n",
    "Y = [[x[0]**2 + x[1]**2] for x in X]\n",
    "\n",
    "epocas = 1000\n",
    "\n",
    "# El nombre de la rede debe ser rede_profunda\n",
    "# your code here\n",
    "rede_profunda = NeuralNetwork(2, 1, [5], learning_rate=0.1)\n",
    "\n",
    "for i in range(epocas):\n",
    "    error = 0\n",
    "    for j in range(len(X)):\n",
    "        # your code here\n",
    "        error += rede_profunda.train(X[j], Y[j])\n",
    "    if i % 100 == 0:\n",
    "        print(\"error:\", error)\n",
    "print(error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificamos el valor predicho vs el valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errores mayor a 0.05: 0\n",
      "Total muestra: 1000\n",
      "PORCENTAJE ACIERTOS: 100.0\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "\n",
    "# Generate training data for f(x, y) = x^2 + y^2\n",
    "X = [[random.uniform(-1, 1), random.uniform(-1, 1)] for _ in range(1000)]\n",
    "Y = [[x[0]**2 + x[1]**2] for x in X]\n",
    "\n",
    "\n",
    "total_sample = len(X)\n",
    "total_errors = 0\n",
    "error_threshold = 0.05\n",
    "\n",
    "for i in range(len(X)):\n",
    "    error = np.abs(rede_profunda.feedforward(X[i])[0] - Y[i][0])\n",
    "    if error > error_threshold:\n",
    "        total_errors += 1\n",
    "\n",
    "print(f'Errores mayor a {error_threshold}: {total_errors}')\n",
    "print(f'Total muestra: {total_sample}')\n",
    "print(f'PORCENTAJE ACIERTOS: {100 * (1 - (total_errors / total_sample))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a68baccfd1250050dedef9816caeaac7",
     "grade": false,
     "grade_id": "cell-2b848c8033fced64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Describe the task here!\n",
    "# Describa todas las decisiones tomadas para tu solución\n",
    "# Esto incluye la cantidad de épocas, tasa de aprendizaje, capas ocultas, capas de entrada y capas de salida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1422e2fec3d53e1e6352e803ebdb6586",
     "grade": false,
     "grade_id": "cell-f741a7d9892acb14",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Describe the task here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, para validar que mi red estuviera bien entrenada, generé un set de pruebas que me permita validar que la red tenga una buena capacidad de generalización sobre datos desconocidos. Lo anterior lo realicé con este código:\n",
    "\n",
    "```python\n",
    "random.seed(1)\n",
    "\n",
    "# Generate training data for f(x, y) = x^2 + y^2\n",
    "X = [[random.uniform(-1, 1), random.uniform(-1, 1)] for _ in range(1000)]\n",
    "Y = [[x[0]**2 + x[1]**2] for x in X]\n",
    "```\n",
    "\n",
    "Luego, las predicciones las realicé ejecutando el método `feedforward` de la red y seteando un nivel de tolerancia del 0.05 de acierto entre la predicción vs el valor real, con el siguiente código.\n",
    "\n",
    "```python\n",
    "total_sample = len(X)\n",
    "total_errors = 0\n",
    "error_threshold = 0.05\n",
    "\n",
    "for i in range(len(X)):\n",
    "    error = np.abs(rede_profunda.feedforward(X[i])[0] - Y[i][0])\n",
    "    if error > error_threshold:\n",
    "        total_errors += 1\n",
    "\n",
    "print(f'Errores mayor a {error_threshold}: {total_errors}')\n",
    "print(f'Total muestra: {total_sample}')\n",
    "print(f'PORCENTAJE ACIERTOS: {100 * (1 - (total_errors / total_sample))}')\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del Learning Rate\n",
    "\n",
    "En segundo lugar, utilicé la configuración inicial de la red, con una capa oculta con 2 neuronas y un `learning_rate` de `0.1`. Para lo anterior, noté que el error se comenzaba a estancar en un valor alto en torno al valor 50, por lo que procedí a disminuirlo a `0.01`, lo cual mejoró el error, pero igual estancándose en torno a 44. Probé con los siguientes valores: `[1, 0.1, 0.01, 0.001, 0.0001]` y noté que con un valor alto como 1, tiende a oscilar el gradiente descendente, por lo que se torna ineficiente la red dado que habría que aumentar las epoch. Con esto consideré un `learning_rate=0.1` dado que puede hacer más eficiente el algoritmo en términos de rapidez de convergencia.\n",
    "\n",
    "Con esta configuración obtuve un porcentaje de aciertos de `8.2 %`. Muy bajo aún."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de capas internas y neuronas\n",
    "\n",
    "En tercer lugar, ya con `learning_rate=0.1`, probé la cantidad de neuronas en la capa interna con los siguientes valores: `[2,3,4,5,6,7]`. Para lo anterior obtuve \n",
    "\n",
    "    (1) NeuralNetwork(2, 1, [3], learning_rate=0.1) -> 92.0 % aciertos\n",
    "    (2) NeuralNetwork(2, 1, [4], learning_rate=0.1) -> 96.7 % aciertos\n",
    "    (3) NeuralNetwork(2, 1, [5], learning_rate=0.1) -> 100 % aciertos\n",
    "    (4) NeuralNetwork(2, 1, [6], learning_rate=0.1) -> 100 % aciertos\n",
    "    (5) NeuralNetwork(2, 1, [7], learning_rate=0.1) -> 100 % aciertos\n",
    "\n",
    "Luego, probé con dos capas internas, por lo que dejé las siguientes configuraciones para `h_layers`: `[1,1], [2,1], [2,2], [2,3], [2,4], [3,1], [3,2], [3,3]`\n",
    "\n",
    "    (6) NeuralNetwork(2, 1, [1,1], learning_rate=0.1) -> 8.6 % aciertos\n",
    "    (7) NeuralNetwork(2, 1, [2,1], learning_rate=0.1) -> 9.09 % aciertos\n",
    "    (8) NeuralNetwork(2, 1, [2,2], learning_rate=0.1) -> 9.19 % aciertos\n",
    "    (9) NeuralNetwork(2, 1, [2,3], learning_rate=0.1) -> 84.2 % aciertos\n",
    "    (10) NeuralNetwork(2, 1, [2,4], learning_rate=0.1) -> 18.4 % aciertos\n",
    "    (11) NeuralNetwork(2, 1, [3,1], learning_rate=0.1) -> 98.5 % aciertos\n",
    "    (12) NeuralNetwork(2, 1, [3,2], learning_rate=0.1) -> 98.5 % aciertos\n",
    "    (13) NeuralNetwork(2, 1, [3,3], learning_rate=0.1) -> 99.0 % aciertos\n",
    "\n",
    "Con lo anterior, vemos que los mejores resultados se obtienen con una sola capa interna, teniendo `5, 6` o `7` neuronas. Además, se puede ver que la incidencia de las neuronas de la primera capa es más positiva en cuanto a la predictibilidad, que en las neuronas de la segunda capa, por lo que podemos considerar que es mejor aumentar las neuronas de la primera capa. Por simplicidad y para prevenir overfitting, nos quedamos temporalmente con la configuración\n",
    "\n",
    "    (3) NeuralNetwork(2, 1, [5], learning_rate=0.1) -> 100 % aciertos "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustes de las épocas\n",
    "\n",
    "Ahora, solo en pos de buscar un modelo más sencillo que el que ya tenemos, compararemos la neurona elegida temporalmente `(3)` con la neurona `(11)`, la que tiene un alto porcentaje de aciertos pero quizás requiere más epochs. Para lo anterior utilizaremos 10.000 veremos cual tendrá los mejores resultados. Si la neurona `(11)` llega a tener 100% de aciertos, la elegiremos dado que tiene menos neuronas en total y podría ser una mejor alternativa a elegir.\n",
    "\n",
    "    10.000 épocas: NeuralNetwork(2, 1, [5], learning_rate=0.1) -> 100 % aciertos\n",
    "    10.000 épocas: NeuralNetwork(2, 1, [3,1], learning_rate=0.1) -> 98.5 % aciertos\n",
    "\n",
    "Vimos que al cabo de la séptima iteración, el error ya se estancaba en torno al `0.273` en la red neuronal `(11)`, por lo que nos quedamos con la red neuronal\n",
    "\n",
    "    NeuralNetwork(2, 1, [5], learning_rate=0.1) + 1.000 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración ideal\n",
    "# epocas = 1000\n",
    "# rede_profunda = NeuralNetwork(2, 1, [5], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "573546c1eada8c60b27f5300df4435af9ba2007194c80719d45c24c6ea4a493c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
