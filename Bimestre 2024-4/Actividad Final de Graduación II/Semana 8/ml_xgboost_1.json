{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv('demand_classification_by_item.csv', sep=';')\n",
    "raw_df = pd.read_csv('df_periodos_rellenados.csv', sep=';')\n",
    "display(raw_df)\n",
    "display(classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.time.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['time'] = pd.to_datetime(raw_df['time'], format=\"%Y-%m-%d\")\n",
    "raw_df['day'] = raw_df.time.dt.day\n",
    "raw_df['month'] = raw_df.time.dt.month\n",
    "raw_df['year'] = raw_df.time.dt.year\n",
    "raw_df['weekday'] = raw_df.time.dt.weekday\n",
    "raw_df['is_weekend'] = (raw_df.weekday >= 5).astype(int)\n",
    "raw_df['days_since_first_data'] = (raw_df.time - raw_df.time.min()).dt.days\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo consideramos los ítems de demanda Lumpy e Intermittent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermittent_ids = classification_df[classification_df.demand_type == 'intermittent']['item'].unique()\n",
    "lumpy_ids = classification_df[classification_df.demand_type == 'lumpy']['item'].unique()\n",
    "raw_df = raw_df[(raw_df['item'].isin(intermittent_ids)) | (raw_df['item'].isin(lumpy_ids))]\n",
    "df = raw_df[['item', 'day', 'month', 'year', 'weekday', 'is_weekend', 'days_since_first_data', 'sales']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos en set de entrenamiento, test y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import pmdarima as pm\n",
    "from darts.models import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['item', 'day', 'month', 'year', 'weekday', 'is_weekend', 'days_since_first_data']]\n",
    "X_test = test_df[['item', 'day', 'month', 'year', 'weekday', 'is_weekend', 'days_since_first_data']]\n",
    "y_train = train_df['sales']\n",
    "y_test = test_df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tamaño de entrenamiento: {len(X_train)}')\n",
    "print(f'Tamaño de prueba: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Escalar características de entrenamiento y prueba\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_pred'] = y_pred\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] != 0.0 or y_test.values[i] != 0:\n",
    "        print(f'y_pred: {y_pred[i]} | y_test: {y_test.values[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir los parámetros a ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Realizar búsqueda en malla con validación cruzada\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Ver los mejores parámetros\n",
    "print(grid_search.best_params_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "model_xgb.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "y_pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mae_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_pred_xgb'] = y_pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df.item.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_pred'] = test_df['y_pred'].apply(lambda x: round(x))\n",
    "test_df['y_pred_xgb'] = test_df['y_pred_xgb'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_rf = mean_absolute_error(test_df['sales'], test_df['y_pred'])\n",
    "mae_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_xgb = mean_absolute_error(test_df['sales'], test_df['y_pred_xgb'])\n",
    "mae_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo de métrica de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classification_df.item.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_dict = classification_df.to_dict()\n",
    "item_price = {}\n",
    "for i in classification_dict['unit_price'].keys():\n",
    "    item_price[classification_dict['item'][i]] = classification_dict['unit_price'][i]\n",
    "\n",
    "item_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el precio unitario a cada item\n",
    "test_df['unit_price'] = test_df['item'].apply(lambda x: item_price[x])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_df = test_df.copy()[['item', 'sales', 'y_pred', 'y_pred_xgb', 'unit_price']]\n",
    "utility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precio promedio productos\n",
    "classification_df['unit_price'].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un costo fijo de inventario para todos los productos\n",
    "# Este costo lo definimos como un 30% del precio promedio de todos los productos\n",
    "STOCK_COST = classification_df['unit_price'].mean() * 0.3\n",
    "STOCK_COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los costos por exceso de inventario por cada modelo\n",
    "def get_stock_cost(row, model):\n",
    "    \"\"\"\n",
    "    Calcula el costo de inventario cuando la cantidad predicha es mayor o igual a la cantidad observada,\n",
    "    según cada modelo.\n",
    "    \"\"\"\n",
    "    if model == 'rf':\n",
    "        target = 'y_pred'\n",
    "    elif model == 'xgb':\n",
    "        target = 'y_pred_xgb'\n",
    "    else:\n",
    "        target = 'y_pred'\n",
    "\n",
    "\n",
    "    if row[target] >= row['sales']:\n",
    "        stock_in_excess = (row[target] - row['sales']) * STOCK_COST\n",
    "    else:\n",
    "        stock_in_excess = 0\n",
    "\n",
    "    return stock_in_excess\n",
    "\n",
    "for m in ['rf', 'xgb']:\n",
    "    utility_df[f'excess_stock_cost_{m}'] = utility_df.apply(lambda x: get_stock_cost(x, m), axis=1)\n",
    "\n",
    "utility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los costos por quiebre de stock por cada modelo\n",
    "def get_stock_out_cost(row, model):\n",
    "    \"\"\"\n",
    "    Calcula el costo de quiebre de stock cuando la cantidad predicha es menor a la cantidad observada,\n",
    "    según cada modelo. Este costo está dado por el precio de venta del item por la cantidad de ítems no vendidos \n",
    "    a causa del quiebre de stock\n",
    "    \"\"\"\n",
    "    if model == 'rf':\n",
    "        target = 'y_pred'\n",
    "    elif model == 'xgb':\n",
    "        target = 'y_pred_xgb'\n",
    "    else:\n",
    "        target = 'y_pred'\n",
    "\n",
    "\n",
    "    if row['sales'] > row[target]:\n",
    "        stock_out = (row['sales'] - row[target]) * row['unit_price']\n",
    "    else:\n",
    "        stock_out = 0\n",
    "\n",
    "    return stock_out\n",
    "\n",
    "for m in ['rf', 'xgb']:\n",
    "    utility_df[f'stock_out_cost_{m}'] = utility_df.apply(lambda x: get_stock_out_cost(x, m), axis=1)\n",
    "\n",
    "utility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los ingresos por venta de repuesto por cada modelo\n",
    "def get_income_earned_by_sale(row, model):\n",
    "    \"\"\"\n",
    "    Calcula el ingreso obtenido por la venta de repuesto\n",
    "    \"\"\"\n",
    "    if model == 'rf':\n",
    "        target = 'y_pred'\n",
    "    elif model == 'xgb':\n",
    "        target = 'y_pred_xgb'\n",
    "    else:\n",
    "        target = 'y_pred'\n",
    "\n",
    "\n",
    "    if row[target] >= row['sales']:\n",
    "        sales = row['sales'] * row['unit_price']\n",
    "    else:\n",
    "        sales = row[target] * row['unit_price']\n",
    "\n",
    "    return sales\n",
    "\n",
    "for m in ['rf', 'xgb']:\n",
    "    utility_df[f'sales_income_{m}'] = utility_df.apply(lambda x: get_income_earned_by_sale(x, m), axis=1)\n",
    "\n",
    "utility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la utilidad para cada dato\n",
    "def get_utility(row, model):\n",
    "    \"\"\"\n",
    "    Calculamos la utilidad para cada dato:\n",
    "    Suma de ingresos - Suma de costos\n",
    "    \"\"\"\n",
    "    total_incomes = row[f'sales_income_{model}']\n",
    "    total_costs = row[f'excess_stock_cost_{model}'] + row[f'stock_out_cost_{model}']\n",
    "    utility = total_incomes - total_costs\n",
    "\n",
    "    return utility\n",
    "\n",
    "for m in ['rf', 'xgb']:\n",
    "    utility_df[f'utility_{m}'] = utility_df.apply(lambda x: get_utility(x, m), axis=1)\n",
    "\n",
    "utility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la utilidad final para cada modelo\n",
    "utility = {}\n",
    "for m in ['rf', 'xgb']:\n",
    "    utility[m] = utility_df[f'utility_{m}'].sum()\n",
    "\n",
    "utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers de series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[raw_df.sales == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df = raw_df[['time', 'item', 'sales']]\n",
    "pivot_df = pd.pivot_table(hf_df, index='time', columns=['item'])\n",
    "pivot_df = pivot_df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pivot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear secuencias de datos\n",
    "def crear_secuencias(datos, n_dias):\n",
    "    X, y = [], []\n",
    "    for i in range(len(datos) - n_dias):\n",
    "        X.append(datos[i:i+n_dias])  # Datos históricos (ventas anteriores)\n",
    "        y.append(datos[i + n_dias])  # El siguiente valor (ventas del día siguiente)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.difference([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el número de días para la secuencia\n",
    "n_dias = 30\n",
    "\n",
    "# Crear las secuencias para cada ítem\n",
    "secuencias = {}\n",
    "for item in items:\n",
    "    item_serie = pivot_df[item].dropna()\n",
    "    secuencias[item] = crear_secuencias(item_serie, n_dias)\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = {}, {}, {}, {}\n",
    "\n",
    "discarded_items = []\n",
    "for item in items:\n",
    "    if len(secuencias[item][0]) > 0 and len(secuencias[item][1]):\n",
    "        X_item, y_item = secuencias[item]\n",
    "        X_train[item], X_test[item], y_train[item], y_test[item] = train_test_split(X_item, y_item, test_size=0.2, shuffle=False)\n",
    "    else:\n",
    "        discarded_items.append(item)\n",
    "\n",
    "items = items.difference(discarded_items)\n",
    "# Ver el tamaño de los datos\n",
    "for item in items:\n",
    "    print(f\"{item} - Entrenamiento: {len(X_train[item])}, Prueba: {len(X_test[item])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "X_train_scaled, X_test_scaled = {}, {}\n",
    "\n",
    "# Escalar los datos de ventas por ítem\n",
    "for item in items:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_scaled[item] = scaler.fit_transform(X_train[item])\n",
    "    X_test_scaled[item] = scaler.transform(X_test[item])\n",
    "    scalers[item] = scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=50, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # La salida de la LSTM\n",
    "        out = self.fc(out[:, -1, :])  # Solo tomamos la última salida de la secuencia\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo para cada ítem\n",
    "models = {}\n",
    "criterions = {}\n",
    "optimizers = {}\n",
    "\n",
    "for item in items:\n",
    "    print(f'Item: {item}')\n",
    "    # Convertir los datos a tensores\n",
    "    X_train_tensor = torch.tensor(X_train_scaled[item], dtype=torch.float32).unsqueeze(-1)\n",
    "    y_train_tensor = torch.tensor(y_train[item], dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled[item], dtype=torch.float32).unsqueeze(-1)\n",
    "    y_test_tensor = torch.tensor(y_test[item], dtype=torch.float32)\n",
    "\n",
    "    # Inicializar el modelo, la función de pérdida y el optimizador\n",
    "    model = LSTMModel(input_size=1, hidden_layer_size=50, output_size=1)\n",
    "    criterion = nn.MSELoss()  # Usamos error cuadrático medio para regresión\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    models[item] = model\n",
    "    criterions[item] = criterion\n",
    "    optimizers[item] = optimizer\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs.squeeze(), y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (epoch+1) % 5 == 0:\n",
    "        #     print(f\"Item: {item}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones y evaluar el rendimiento\n",
    "predictions = {}\n",
    "for item in items:\n",
    "    model = models[item]\n",
    "    X_test_tensor = torch.tensor(X_test_scaled[item], dtype=torch.float32).unsqueeze(-1)\n",
    "    y_test_tensor = torch.tensor(y_test[item], dtype=torch.float32)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "\n",
    "    # Convertir predicciones y valores reales a numpy\n",
    "    y_pred = y_pred_tensor.squeeze().numpy()\n",
    "    y_test_actual = y_test_tensor.numpy()\n",
    "\n",
    "    # Calcular el error absoluto medio (MAE)\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "    print(f\"Item: {item}, MAE: {mae:.2f}\")\n",
    "\n",
    "    predictions[item] = {\n",
    "        'mae': mae,\n",
    "        'y_pred': y_pred,\n",
    "        'y_test': y_test_actual\n",
    "    }\n",
    "\n",
    "    # Graficar las predicciones vs las ventas reales\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(df.index[-len(y_test_actual):], y_test_actual, label='Ventas reales', color='blue')\n",
    "    # plt.plot(df.index[-len(y_pred):], y_pred, label='Predicciones', color='red', linestyle='--')\n",
    "    # plt.legend()\n",
    "    # plt.title(f'Predicción de ventas con LSTM - {item}')\n",
    "    # plt.xlabel('Fecha')\n",
    "    # plt.ylabel('Ventas')\n",
    "    # plt.xticks(rotation=45)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = [predictions[p]['mae'] for p in predictions]\n",
    "np.mean(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = list(predictions.keys())\n",
    "len(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[5432]['y_pred']\n",
    "predictions[5432]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_pred'] = test_df['y_pred'].apply(lambda x: round(x))\n",
    "test_df['y_pred_xgb'] = test_df['y_pred_xgb'].apply(lambda x: round(x))\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_pred_lstm'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
